---
# Source: secret-sharer-app/templates/networkpolicies.yaml
# templates/networkpolicies.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: ss-mvp-secret-sharer-app-default-deny-all
  namespace: default
  labels:
    helm.sh/chart: secret-sharer-app-0.1.0
    app.kubernetes.io/name: secret-sharer-app
    app.kubernetes.io/instance: ss-mvp
    app.kubernetes.io/version: "0.1.0"
    app.kubernetes.io/managed-by: Helm
spec:
  # Apply this policy to all pods in the namespace
  # by selecting no specific pods (empty podSelector).
  podSelector: {} # An empty podSelector selects all pods in the namespace
  policyTypes:
    - Ingress # Apply to incoming traffic
    - Egress  # Apply to outgoing traffic
  # No ingress rules defined means all ingress traffic is denied by default.
  # No egress rules defined means all egress traffic is denied by default.
  # ingress: [] # Explicitly showing empty, usually omitted for default deny
  # egress: []  # Explicitly showing empty, usually omitted for default deny
---
# Source: secret-sharer-app/templates/networkpolicies.yaml
# Allow traffic from Frontend pods to Backend pods on the backend's service port
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: ss-mvp-secret-sharer-app-allow-frontend-to-backend
  namespace: default
  labels:
    helm.sh/chart: secret-sharer-app-0.1.0
    app.kubernetes.io/name: secret-sharer-app
    app.kubernetes.io/instance: ss-mvp
    app.kubernetes.io/version: "0.1.0"
    app.kubernetes.io/managed-by: Helm
spec:
  # This policy applies to Backend pods
  podSelector:
    matchLabels:
      app.kubernetes.io/name: secret-sharer-app
      app.kubernetes.io/instance: ss-mvp
      app.kubernetes.io/component: backend # Selects the backend pods
  policyTypes:
    - Ingress # This policy is about allowing INCOMING traffic to the backend
  ingress:
    - from:
        # Allow traffic FROM pods that have these labels (i.e., Frontend pods)
        - podSelector:
            matchLabels:
              app.kubernetes.io/name: secret-sharer-app
              app.kubernetes.io/instance: ss-mvp
              app.kubernetes.io/component: frontend # Selects the frontend pods
      ports:
        # Allow traffic TO this port on the Backend pods
        - protocol: TCP
          port: 8000 # Port the backend listens on (e.g., 8000)
---
# Source: secret-sharer-app/templates/networkpolicies.yaml
# Allow traffic from Backend pods to Database pods on the database's service port
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: ss-mvp-secret-sharer-app-allow-backend-to-database
  namespace: default
  labels:
    helm.sh/chart: secret-sharer-app-0.1.0
    app.kubernetes.io/name: secret-sharer-app
    app.kubernetes.io/instance: ss-mvp
    app.kubernetes.io/version: "0.1.0"
    app.kubernetes.io/managed-by: Helm
spec:
  # This policy applies to Database pods
  podSelector:
    matchLabels:
      app.kubernetes.io/name: secret-sharer-app
      app.kubernetes.io/instance: ss-mvp
      app.kubernetes.io/component: database # Selects the database pods
  policyTypes:
    - Ingress # This policy is about allowing INCOMING traffic to the database
  ingress:
    - from:
        # Allow traffic FROM pods that have these labels (i.e., Backend pods)
        - podSelector:
            matchLabels:
              app.kubernetes.io/name: secret-sharer-app
              app.kubernetes.io/instance: ss-mvp
              app.kubernetes.io/component: backend # Selects the backend pods
      ports:
        # Allow traffic TO this port on the Database pods
        - protocol: TCP
          port: 5432 # Standard PostgreSQL port, ensure this matches your DB containerPort
---
# Source: secret-sharer-app/templates/networkpolicies.yaml
# Allow Ingress traffic to Frontend pods (e.g., from an Ingress Controller)
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: ss-mvp-secret-sharer-app-allow-ingress-to-frontend
  namespace: default
  labels:
    helm.sh/chart: secret-sharer-app-0.1.0
    app.kubernetes.io/name: secret-sharer-app
    app.kubernetes.io/instance: ss-mvp
    app.kubernetes.io/version: "0.1.0"
    app.kubernetes.io/managed-by: Helm
spec:
  # This policy applies to Frontend pods
  podSelector:
    matchLabels:
      app.kubernetes.io/name: secret-sharer-app
      app.kubernetes.io/instance: ss-mvp
      app.kubernetes.io/component: frontend # Selects the frontend pods
  policyTypes:
    - Ingress # This policy is about allowing INCOMING traffic to the frontend
  ingress:
    - from:
        # This is a general approach. For AGIC, traffic originates from the Application Gateway.
        # A more precise rule would use an ipBlock for the App Gateway's subnet.
        # For MVP, allowing from common Ingress controller namespaces or all within the cluster (if no other from specified)
        # after default deny is a starting point.
        # Let's allow from any pod in the 'kube-system' namespace (where AGIC controller runs)
        # AND from any pod within the same namespace (for potential same-namespace routing/forwarding)
        # This may need to be adjusted based on the actual Ingress controller behavior and network setup.
        # If your Ingress controller (like Nginx) runs in a different specific namespace, adjust accordingly.
        - namespaceSelector:
            matchLabels:
              # This label is common on the kube-system namespace.
              # You might need to verify/adjust for your specific AKS setup or Ingress controller namespace.
              kubernetes.io/metadata.name: kube-system 
        # If you have an Ingress controller in the same namespace as your app (not typical for AGIC addon):
        # - podSelector: {} # Allows from any pod in the same namespace (if that's where your Ingress controller proxies from)
      ports:
        # Allow traffic TO this port on the Frontend pods
        - protocol: TCP
          port: 80 # Port the frontend (Nginx) listens on (e.g., 80)
---
# Source: secret-sharer-app/templates/database-serviceaccount.yaml
# templates/database-serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: secret-sharer-db-init-sa 
  labels:
    helm.sh/chart: secret-sharer-app-0.1.0
    app.kubernetes.io/name: secret-sharer-app
    app.kubernetes.io/instance: ss-mvp
    app.kubernetes.io/version: "0.1.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: database
  annotations:
    # This annotation links the SA to the Azure User Assigned Managed Identity via Workload Identity
    azure.workload.identity/client-id: "ef9e4258-98dc-49a8-804b-dfe502309386"
automountServiceAccountToken: true # Required for Workload Identity token projection
---
# Source: secret-sharer-app/templates/serviceaccount.yaml
# templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  # Use the name defined in backend.serviceAccount.name or default to a helper-generated name if not specified
  name: secret-sharer-backend-sa
  labels:
    helm.sh/chart: secret-sharer-app-0.1.0
    app.kubernetes.io/name: secret-sharer-app
    app.kubernetes.io/instance: ss-mvp
    app.kubernetes.io/version: "0.1.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: backend # Add component label for consistency
  annotations:
    # This annotation links the SA to the Azure User Assigned Managed Identity via Workload Identity
    azure.workload.identity/client-id: "fa376030-252d-443f-a32d-294e3cda90e1"
automountServiceAccountToken: true # Required for Workload Identity token projection
---
# Source: secret-sharer-app/templates/database-configmap.yaml
# templates/database-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: ss-mvp-secret-sharer-app-db-init-script
  labels:
    helm.sh/chart: secret-sharer-app-0.1.0
    app.kubernetes.io/name: secret-sharer-app
    app.kubernetes.io/instance: ss-mvp
    app.kubernetes.io/version: "0.1.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: database
data:
  # The key name here (e.g., init-db.sh) will become the filename when mounted as a volume
  init-db.sh: |-
    #!/bin/bash
    set -e # Exit immediately if a command exits with a non-zero status.
    
    # Variables
    # POSTGRES_USER, POSTGRES_PASSWORD, POSTGRES_DB are automatically available from container env vars
    APP_DB_USER_ENV_VAR="APP_DB_USER" # Env var for the app username (still needed)
    # Path where the CSI driver mounts the app password file, based on objectAlias in SPC
    APP_DB_PASSWORD_FILE="/mnt/db-secrets-store/appDbPasswordKeyVault"
    
    echo "Starting database initialization script..."
    
    # Get App Username from Env Var (sourced from values.yaml in StatefulSet)
    APP_DB_USER="${!APP_DB_USER_ENV_VAR}"
    
    # Check if required inputs are available
    if [ -z "$APP_DB_USER" ]; then
      echo "Error: Required environment variable APP_DB_USER is not set."
      exit 1
    fi
    if [ ! -f "$APP_DB_PASSWORD_FILE" ]; then
      echo "Error: Application password file '$APP_DB_PASSWORD_FILE' not found. Check SecretProviderClass and volume mount."
      exit 1
    fi
    
    # Read the password from the mounted file
    APP_DB_PASSWORD=$(cat "$APP_DB_PASSWORD_FILE")
    if [ -z "$APP_DB_PASSWORD" ]; then
        echo "Error: Application password file '$APP_DB_PASSWORD_FILE' is empty."
        exit 1
    fi
    
    echo "Read application username and password successfully."
    
    # Use psql to execute SQL commands
    psql -v ON_ERROR_STOP=1 --username "$POSTGRES_USER" --dbname "$POSTGRES_DB" <<-EOSQL
        -- Create the application user if it doesn't exist
        DO
        \$do\$
        BEGIN
           IF NOT EXISTS (
              SELECT FROM pg_catalog.pg_roles
              WHERE  rolname = '$APP_DB_USER') THEN
    
              -- Use password read from file
              CREATE ROLE "$APP_DB_USER" WITH LOGIN PASSWORD '$APP_DB_PASSWORD';
              RAISE NOTICE 'Role "$APP_DB_USER" created.';
           ELSE
              RAISE NOTICE 'Role "$APP_DB_USER" already exists. Skipping creation.';
              -- Optionally update password if needed:
              -- ALTER ROLE "$APP_DB_USER" WITH PASSWORD '$APP_DB_PASSWORD';
           END IF;
        END
        \$do\$;
    
        -- Grant connect permissions
        GRANT CONNECT ON DATABASE "$POSTGRES_DB" TO "$APP_DB_USER";
        RAISE NOTICE 'CONNECT permission granted to "$APP_DB_USER" on database "$POSTGRES_DB".';
    
        -- Grant schema usage
        GRANT USAGE ON SCHEMA public TO "$APP_DB_USER";
        RAISE NOTICE 'USAGE permission granted to "$APP_DB_USER" on schema public.';
    
        -- Grant default privileges for future tables/sequences
        ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT, INSERT, UPDATE, DELETE ON TABLES TO "$APP_DB_USER";
        RAISE NOTICE 'Default table permissions granted to "$APP_DB_USER" in schema public.';
        -- ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT USAGE, SELECT ON SEQUENCES TO "$APP_DB_USER";
        -- RAISE NOTICE 'Default sequence permissions granted to "$APP_DB_USER" in schema public.';
    
        -- Grant permissions on the specific 'secrets' table if it exists now
        DO \$\$
        BEGIN
           IF EXISTS (SELECT FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'secrets') THEN
              GRANT SELECT, INSERT, UPDATE, DELETE ON TABLE public.secrets TO "$APP_DB_USER";
              RAISE NOTICE 'Permissions granted to "$APP_DB_USER" on existing table public.secrets.';
           END IF;
        END
        \$\$;
    
        SELECT 'Database initialization script completed successfully.' AS status;
    EOSQL
    
    # --- SECURITY: Clear the password variable ---
    unset APP_DB_PASSWORD
    
    echo "Database initialization script finished."
---
# Source: secret-sharer-app/templates/rbac.yaml
# templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: ss-mvp-secret-sharer-app-backend-role
  namespace: default # Role is namespaced
  labels:
    helm.sh/chart: secret-sharer-app-0.1.0
    app.kubernetes.io/name: secret-sharer-app
    app.kubernetes.io/instance: ss-mvp
    app.kubernetes.io/version: "0.1.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: backend
rules:
  # Permissions for the backend pod to read the Kubernetes Secret
  # created by its SecretProviderClass.
  - apiGroups: [""] # Core API group for Secrets
    resources: ["secrets"]
    # Restrict to the specific secret name used by the backend deployment
    resourceNames: ["ss-mvp-secret-sharer-app-backend-secrets"]
    verbs: ["get", "watch"] # "watch" allows updates if the secret content changes
---
# Source: secret-sharer-app/templates/rbac.yaml
# Separator for the next resource in the same file
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: ss-mvp-secret-sharer-app-backend-rolebinding
  namespace: default # RoleBinding is also namespaced
  labels:
    helm.sh/chart: secret-sharer-app-0.1.0
    app.kubernetes.io/name: secret-sharer-app
    app.kubernetes.io/instance: ss-mvp
    app.kubernetes.io/version: "0.1.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: backend
subjects:
  - kind: ServiceAccount
    # Name of the ServiceAccount for the backend
    name: secret-sharer-backend-sa
    # namespace: default # Not needed if SA is in the same namespace as RoleBinding
roleRef:
  kind: Role # Must be Role (not ClusterRole) for a namespaced RoleBinding
  name: ss-mvp-secret-sharer-app-backend-role # Name of the Role created above
  apiGroup: rbac.authorization.k8s.io
---
# Source: secret-sharer-app/templates/backend-service.yaml
# templates/backend-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: ss-mvp-secret-sharer-app-backend
  labels:
    helm.sh/chart: secret-sharer-app-0.1.0
    app.kubernetes.io/name: secret-sharer-app
    app.kubernetes.io/instance: ss-mvp
    app.kubernetes.io/version: "0.1.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: backend
spec:
  type: ClusterIP
  ports:
    - port: 8000
      targetPort: http # Matches the 'name' of the port in the Deployment's container spec
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: secret-sharer-app
    app.kubernetes.io/instance: ss-mvp
    app.kubernetes.io/component: backend
---
# Source: secret-sharer-app/templates/database-service.yaml
# templates/database-service.yaml

# Headless service for StatefulSet DNS resolution
apiVersion: v1
kind: Service
metadata:
  name: ss-mvp-secret-sharer-app-db-headless
  labels:
    helm.sh/chart: secret-sharer-app-0.1.0
    app.kubernetes.io/name: secret-sharer-app
    app.kubernetes.io/instance: ss-mvp
    app.kubernetes.io/version: "0.1.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: database
spec:
  ports:
    - name: postgres
      port: 5432
      targetPort: postgres
      protocol: TCP
  clusterIP: None # Makes it a headless service
  selector:
    app.kubernetes.io/name: secret-sharer-app
    app.kubernetes.io/instance: ss-mvp
    app.kubernetes.io/component: database
---
# Source: secret-sharer-app/templates/database-service.yaml
# ClusterIP service for applications to connect to
apiVersion: v1
kind: Service
metadata:
  name: ss-mvp-secret-sharer-app-db
  labels:
    helm.sh/chart: secret-sharer-app-0.1.0
    app.kubernetes.io/name: secret-sharer-app
    app.kubernetes.io/instance: ss-mvp
    app.kubernetes.io/version: "0.1.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: database
spec:
  type: ClusterIP
  ports:
    - name: postgres
      port: 5432
      targetPort: postgres
      protocol: TCP
  selector:
    app.kubernetes.io/name: secret-sharer-app
    app.kubernetes.io/instance: ss-mvp
    app.kubernetes.io/component: database
---
# Source: secret-sharer-app/templates/frontend-service.yaml
# templates/frontend-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: ss-mvp-secret-sharer-app-frontend
  labels:
    helm.sh/chart: secret-sharer-app-0.1.0
    app.kubernetes.io/name: secret-sharer-app
    app.kubernetes.io/instance: ss-mvp
    app.kubernetes.io/version: "0.1.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: frontend
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: http # Matches the 'name' of the port in the Deployment's container spec
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: secret-sharer-app
    app.kubernetes.io/instance: ss-mvp
    app.kubernetes.io/component: frontend
---
# Source: secret-sharer-app/templates/backend-deployment.yaml
# templates/backend-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ss-mvp-secret-sharer-app-backend
  labels:
    helm.sh/chart: secret-sharer-app-0.1.0
    app.kubernetes.io/name: secret-sharer-app
    app.kubernetes.io/instance: ss-mvp
    app.kubernetes.io/version: "0.1.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: backend
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: secret-sharer-app
      app.kubernetes.io/instance: ss-mvp
      app.kubernetes.io/component: backend
  template:
    metadata:
      labels:
        app.kubernetes.io/name: secret-sharer-app
        app.kubernetes.io/instance: ss-mvp
        app.kubernetes.io/component: backend
    spec:
      serviceAccountName: secret-sharer-backend-sa
      automountServiceAccountToken: true # Required for Workload Identity if SA is specified
      containers:
        - name: backend
          image: "acrsecuresecsharer.azurecr.io/secure-secret-sharer-backend:0.1.0"
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 8000 # Assuming backend runs on port 8000 internally
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /health # Or your specific health check endpoint e.g., /healthz
              port: http # Refers to the containerPort named 'http' (e.g., 8000)
            initialDelaySeconds: 15
            periodSeconds: 20
            timeoutSeconds: 5
            failureThreshold: 3
            successThreshold: 1
          readinessProbe:
            httpGet:
              path: / # Or your specific health check endpoint e.g., /healthz
              port: http # Refers to the containerPort named 'http'
            initialDelaySeconds: 5
            periodSeconds: 15
            timeoutSeconds: 5
            failureThreshold: 3
            successThreshold: 1
          resources:
            limits:
              cpu: 500m # 0.5 CPU core
              memory: 256Mi
            requests:
              cpu: 100m # 0.1 CPU core
              memory: 128Mi
          env:
            # These will be populated from the Kubernetes secret created by the SecretProviderClass
            - name: MASTER_ENCRYPTION_KEY
              valueFrom:
                secretKeyRef:
                  name: ss-mvp-secret-sharer-app-backend-secrets
                  key: appMasterEncryptionKey # Key name within the K8s Secret
            - name: DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: ss-mvp-secret-sharer-app-backend-secrets
                  key: dbUser # Key name within the K8s Secret
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: ss-mvp-secret-sharer-app-backend-secrets
                  key: dbPassword # Key name within the K8s Secret
            - name: DATABASE_NAME
              value: "secret_sharer_db" # From database values
            - name: DATABASE_HOST
              # This will be the ClusterIP service name of your database
              value: ss-mvp-secret-sharer-app-db 
            - name: DATABASE_PORT
              value: "5432" # Default PostgreSQL port
            # Your Python app will need to construct the DATABASE_URL from these individual components
            # Alternatively, the SecretProviderClass could create a secret with a fully formed DATABASE_URL if preferred.

          # We will add livenessProbe and readinessProbe later
          # We will add resources (requests/limits) later
          volumeMounts:
            - name: secrets-store-inline
              mountPath: "/mnt/secrets-store" # Secrets will be mounted here as files by CSI driver
              readOnly: true
      volumes:
        - name: secrets-store-inline
          csi:
            driver: secrets-store.csi.k8s.io
            readOnly: true
            volumeAttributes:
              secretProviderClass: ss-mvp-secret-sharer-app-backend-spc
            # nodePublishSecretRef: # Only needed if Azure Key Vault firewall is enabled for specific service endpoints
            #   name: ""
---
# Source: secret-sharer-app/templates/frontend-deployment.yaml
# templates/frontend-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ss-mvp-secret-sharer-app-frontend
  labels:
    helm.sh/chart: secret-sharer-app-0.1.0
    app.kubernetes.io/name: secret-sharer-app
    app.kubernetes.io/instance: ss-mvp
    app.kubernetes.io/version: "0.1.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: frontend
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: secret-sharer-app
      app.kubernetes.io/instance: ss-mvp
      app.kubernetes.io/component: frontend
  template:
    metadata:
      labels:
        app.kubernetes.io/name: secret-sharer-app
        app.kubernetes.io/instance: ss-mvp
        app.kubernetes.io/component: frontend
    spec:
      containers:
        - name: frontend
          image: "acrsecuresecsharer.azurecr.io/secure-secret-sharer-frontend:0.1.0"
          imagePullPolicy: IfNotPresent
          ports:
            - name: http # Name of the port
              containerPort: 80 # Nginx typically listens on port 80
              protocol: TCP
          livenessProbe:
            httpGet:
              path: / # Root path should return 200 OK for Nginx
              port: http # Refers to the containerPort named 'http'
            initialDelaySeconds: 10
            periodSeconds: 15
            timeoutSeconds: 5
            failureThreshold: 3
            successThreshold: 1
          readinessProbe:
            httpGet:
              path: / # Root path should return 200 OK for Nginx
              port: http # Refers to the containerPort named 'http'
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 3
            successThreshold: 1
          resources:
            limits:
              cpu: 200m # 0.2 CPU core
              memory: 128Mi
            requests:
              cpu: 50m  # 0.05 CPU core
              memory: 64Mi
---
# Source: secret-sharer-app/templates/database-statefulset.yaml
# templates/database-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: ss-mvp-secret-sharer-app-db
  labels:
    helm.sh/chart: secret-sharer-app-0.1.0
    app.kubernetes.io/name: secret-sharer-app
    app.kubernetes.io/instance: ss-mvp
    app.kubernetes.io/version: "0.1.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: database
spec:
  serviceName: ss-mvp-secret-sharer-app-db-headless # Headless service for StatefulSet
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: secret-sharer-app
      app.kubernetes.io/instance: ss-mvp
      app.kubernetes.io/component: database
  template:
    metadata:
      labels:
        app.kubernetes.io/name: secret-sharer-app
        app.kubernetes.io/instance: ss-mvp
        app.kubernetes.io/component: database
    spec:
      serviceAccountName: secret-sharer-db-init-sa
      automountServiceAccountToken: true # Ensure token is mounted for WI/CSI Driver
      
      terminationGracePeriodSeconds: 10
      # We will add securityContext for the pod later
      containers:
        - name: postgres
          image: "postgres:15.7-alpine3.20"
          imagePullPolicy: IfNotPresent
          ports:
            - name: postgres
              containerPort: 5432
              protocol: TCP
          livenessProbe:
            tcpSocket:
              port: postgres # Refers to the containerPort named 'postgres' (5432)
            # Or, for a more thorough check, an exec probe (requires psql in the image):
            # exec:
            #   command:
            #   - sh
            #   - -c
            #   - PGPASSWORD=$POSTGRES_PASSWORD psql -h 127.0.0.1 -U $POSTGRES_USER -d $POSTGRES_DB -c 'SELECT 1'
            initialDelaySeconds: 30
            periodSeconds: 30
            timeoutSeconds: 5
            failureThreshold: 3
            successThreshold: 1
          readinessProbe:
            tcpSocket: # A TCP check is usually sufficient for DB readiness
              port: postgres # Refers to the containerPort named 'postgres'
            # Or, an exec probe like the liveness one if preferred:
            # exec:
            #   command: ["sh", "-c", "PGPASSWORD=$POSTGRES_PASSWORD psql -h 127.0.0.1 -U $POSTGRES_USER -d $POSTGRES_DB -c 'SELECT 1'"]
            initialDelaySeconds: 10 # Shorter delay for readiness after it's live
            periodSeconds: 15
            timeoutSeconds: 5
            failureThreshold: 3
            successThreshold: 1
          resources:
            limits:
              cpu: 1000m # 1 CPU core
              memory: 1Gi # 1 GiB Memory
            requests:
              cpu: 250m # 0.25 CPU core
              memory: 512Mi # 0.5 GiB Memory
          # ... env, volumeMounts ...
          env:
            - name: POSTGRES_DB
              value: "secret_sharer_db"
               # Set the user explicitly, typically 'postgres' for the standard image's init process
            - name: POSTGRES_USER
              value: "postgres" # Standard user for initial setup
            # Fetch the password from the K8s secret created by the db-init-spc
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: ss-mvp-secret-sharer-app-db-init-credentials # K8s Secret synced by db-init-spc
                  key: POSTGRES_PASSWORD # Key within that K8s Secret

            # IMPORTANT: Database user and password will come from Azure Key Vault via CSI Driver
            # We will create Kubernetes secrets from the CSI-mounted secrets,
            # and then reference those secrets here using valueFrom.secretKeyRef
            # For now, this is a placeholder section for where those env vars would go.
            # We'll fully implement this in a later step (Task 5.1)
            - name: PGDATA
              value: /var/lib/postgresql/data/pgdata
            # Example of how user/password MIGHT be set IF NOT using CSI driver (NOT RECOMMENDED FOR PROD)
            # - name: POSTGRES_USER
            #   valueFrom:
            #     secretKeyRef:
            #       name: ss-mvp-secret-sharer-app-db-credentials
            #       key: username
            # - name: POSTGRES_PASSWORD
            #   valueFrom:
            #     secretKeyRef:
            #       name: ss-mvp-secret-sharer-app-db-credentials
            #       key: password

            # These values MUST match the secrets stored in AKV for the backend
            - name: APP_DB_USER
              value: "app-db-user" # Get desired app username from backend's expected AKV secret name
          volumeMounts:
            - name: db-data
              mountPath: /var/lib/postgresql/data
            
            - name: db-secrets-store-inline
              mountPath: "/mnt/db-secrets-store" # Different mount path from backend
              readOnly: true

             # Mount the init script from the ConfigMap to create the database user
            - name: db-init-script-volume # Matches volume name below
              mountPath: /docker-entrypoint-initdb.d # Standard path for postgres init scripts
              readOnly: true 
          
      # Define the CSI Secrets Store Volume for the DB Init SPC ---
      volumes:
        - name: db-secrets-store-inline
          csi:
            driver: secrets-store.csi.k8s.io
            readOnly: true
            volumeAttributes:
              secretProviderClass: ss-mvp-secret-sharer-app-db-init-spc # Reference the DB SPC
                # Define the volume sourcing from the ConfigMap ---
        - name: db-init-script-volume
          configMap:
            name: ss-mvp-secret-sharer-app-db-init-script 
            # Make the script executable
            defaultMode: 0755 # rwxr-xr-x permissions

  volumeClaimTemplates:
    - metadata:
        name: db-data
      spec:
        accessModes: [ "ReadWriteOnce" ]
        resources:
          requests:
            storage: 1Gi
        # storageClassName: default # Uncomment if you need a specific storage class
---
# Source: secret-sharer-app/templates/ingress.yaml
# templates/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ss-mvp-secret-sharer-app
  labels:
    helm.sh/chart: secret-sharer-app-0.1.0
    app.kubernetes.io/name: secret-sharer-app
    app.kubernetes.io/instance: ss-mvp
    app.kubernetes.io/version: "0.1.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    # Specify AGIC as the Ingress Class
    kubernetes.io/ingress.class: azure/application-gateway

    # Example: If you enable TLS below and want App Gateway to handle SSL redirection

    # Add other AGIC-specific annotations from .Values.ingress.annotations if needed
spec:
  rules:
    - host: "secretsharer.example.com"
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: ss-mvp-secret-sharer-app-frontend # Targets the frontend service
                port:
                  number: 80 # Port of the frontend service
---
# Source: secret-sharer-app/templates/backend-secretproviderclass.yaml
# templates/backend-secretproviderclass.yaml
apiVersion: secrets-store.csi.x-k8s.io/v1
kind: SecretProviderClass
metadata:
  name: ss-mvp-secret-sharer-app-backend-spc
  labels:
    helm.sh/chart: secret-sharer-app-0.1.0
    app.kubernetes.io/name: secret-sharer-app
    app.kubernetes.io/instance: ss-mvp
    app.kubernetes.io/version: "0.1.0"
    app.kubernetes.io/managed-by: Helm
spec:
  provider: azure
  parameters:
    usePodIdentity: "false" # We will use Workload Identity; if true, it refers to the older Pod Identity v1
    useVMManagedIdentity: "false" # Not using VM managed identity directly for this
    clientID: "fa376030-252d-443f-a32d-294e3cda90e1" 
    keyvaultName: "kv-secure-secret-sharer" 
    cloudName: "" # Optional: AzurePublicCloud, AzureChinaCloud, etc. Defaults to AzurePublicCloud
    objects: |
      array:
        - |
          objectName: app-db-user
          objectType: secret
          objectAlias: dbUserKeyVault # Alias for the mounted file, also used as key in K8s secret if synced
        - |
          objectName: app-db-password
          objectType: secret
          objectAlias: dbPasswordKeyVault
        - |
          objectName: app-master-encryption-key
          objectType: secret
          objectAlias: appMasterKeyKeyVault
    tenantId: "6e05f665-11c4-4221-9eea-3065ede81619" # Your Azure Tenant ID

  # This section instructs the driver to create/update a Kubernetes Secret
  # with the fetched values. This is often easier for applications to consume.
  secretObjects:
    - secretName: ss-mvp-secret-sharer-app-backend-secrets # Name of the K8s Secret to create
      type: Opaque
      data:
        # These 'objectName' fields here refer to the 'objectAlias' from the 'objects' array above
        - objectName: dbUserKeyVault 
          key: dbUser # The key name in the Kubernetes Secret
        - objectName: dbPasswordKeyVault
          key: dbPassword # The key name in the Kubernetes Secret
        - objectName: appMasterKeyKeyVault
          key: appMasterEncryptionKey # The key name in the Kubernetes Secret
---
# Source: secret-sharer-app/templates/database-secretproviderclass.yaml
# templates/database-secretproviderclass.yaml
apiVersion: secrets-store.csi.x-k8s.io/v1
kind: SecretProviderClass
metadata:
  name: ss-mvp-secret-sharer-app-db-init-spc # Distinct name for DB SPC
  labels:
    helm.sh/chart: secret-sharer-app-0.1.0
    app.kubernetes.io/name: secret-sharer-app
    app.kubernetes.io/instance: ss-mvp
    app.kubernetes.io/version: "0.1.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: database
spec:
  provider: azure
  parameters:
    usePodIdentity: "false" # Required for Workload Identity
    useVMManagedIdentity: "false" # Not using VM identity directly
    # Uses the Client ID of the *Database* User Assigned Managed Identity
    clientID: "ef9e4258-98dc-49a8-804b-dfe502309386"
    keyvaultName: "kv-secure-secret-sharer" # Assuming same Key Vault for all secrets
    cloudName: "" # Defaults to AzurePublicCloud
    objects: |
      array:
        - |
          objectName: postgres-password # Name of the secret in AKV for postgres init password
          objectType: secret
          objectAlias: postgresPasswordKeyVault # Alias for the mounted file & K8s secret key
        # Fetch the application user's password ---
        - |
          objectName: app-db-password # Fetch the app password secret name
          objectType: secret
          objectAlias: appDbPasswordKeyVault # Alias for the *mounted file*

    tenantId: "6e05f665-11c4-4221-9eea-3065ede81619" # Assuming same tenant ID

  # Sync the fetched secret into a Kubernetes Secret for the DB pod to use
  secretObjects:
    - secretName: ss-mvp-secret-sharer-app-db-init-credentials # K8s secret name
      type: Opaque
      data:
        # POSTGRES_USER is typically 'postgres' by default and can often be set directly
        # in the StatefulSet env vars. We primarily need the password from Key Vault.
        - objectName: postgresPasswordKeyVault # Alias from objects array
          key: POSTGRES_PASSWORD # Key name in the K8s Secret, matching expected env var
